{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "VkdhTgdCgOt2"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(f\"first 10 words{words[:10]}\")\n",
        "print(f\"length of words: {len(words)}\")\n",
        "print(f\"Min word length: {min(len(w) for w in words)}\")\n",
        "print(f\"Max word length: {max(len(w) for w in words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IMTgYLnjHWQ",
        "outputId": "f5c05067-9d30-423b-ef86-62be48f8a415"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first 10 words['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']\n",
            "length of words: 32033\n",
            "Min word length: 2\n",
            "Max word length: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E01: Train a trigram language model"
      ],
      "metadata": {
        "id": "0Bl-J8D5i_6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a list of chars\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "chars = ['.'] + chars\n",
        "\n",
        "# Dictionnary char to index\n",
        "stoi = {s:i for i,s in enumerate(chars)}\n",
        "# Dictionnary index to char\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "id": "AjFG9SYBi-sp"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Counting"
      ],
      "metadata": {
        "id": "5Cb2O8uzrPtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = torch.ones(27, 27, 27, dtype=torch.int32) # Ones for smoothing\n",
        "N[0,0,0] = 0\n",
        "\n",
        "for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        ix3 = stoi[ch3]\n",
        "        N[ix1, ix2, ix3] += 1\n",
        "\n",
        "P = N.float()\n",
        "P /= P.sum(dim=2, keepdim=True)"
      ],
      "metadata": {
        "id": "s-SfXfFerPdf"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def myloss(words):\n",
        "  log_likelihood = 0.0\n",
        "  n = 0\n",
        "\n",
        "  for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "      ix1 = stoi[ch1]\n",
        "      ix2 = stoi[ch2]\n",
        "      ix3 = stoi[ch3]\n",
        "\n",
        "      prob = P[ix1, ix2, ix3]\n",
        "      logprob = torch.log(prob)\n",
        "      log_likelihood += logprob\n",
        "      n += 1\n",
        "\n",
        "  print(f'{log_likelihood=}')\n",
        "  nll = -log_likelihood\n",
        "  print(f'{nll=}')\n",
        "  print(f'{nll/n=}')\n",
        "\n",
        "myloss(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk0kozeAsx3Y",
        "outputId": "15ac940e-2b92-435e-c20a-f733eca26abb"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log_likelihood=tensor(-410414.9688)\n",
            "nll=tensor(410414.9688)\n",
            "nll/n=tensor(2.0927)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling names\n",
        "names = []\n",
        "for i in range(10):\n",
        "  out = []\n",
        "  ix1, ix2 = 0, 0\n",
        "  while True:\n",
        "    p = P[ix1, ix2]\n",
        "    ix1 = ix2\n",
        "    ix2 = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
        "    if ix2 == 0:\n",
        "      break\n",
        "    out.append(itos[ix2])\n",
        "  names.append(\"\".join(out))\n",
        "\n",
        "print(names)\n",
        "myloss(names)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WmOE2Stvd2j",
        "outputId": "67c84989-d511-4bd8-e5f9-f2b7e253305e"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['en', 'kelyfxmyasio', 'ni', 'ulacqrniseni', 'dachah', 'grakaidayantriasel', 'teonn', 'mwrne', 'ma', 'eyarya']\n",
            "log_likelihood=tensor(-171.5460)\n",
            "nll=tensor(171.5460)\n",
            "nll/n=tensor(2.4507)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Neural Net"
      ],
      "metadata": {
        "id": "q7UVERrs9Q7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training set\n",
        "xs, ys = [], []\n",
        "\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "    ix1 = stoi[ch1]\n",
        "    ix2 = stoi[ch2]\n",
        "    ix3 = stoi[ch3]\n",
        "\n",
        "    xs.append([ix1, ix2])\n",
        "    ys.append(ix3)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)"
      ],
      "metadata": {
        "id": "lpoljMNK9bQF"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27*2, 27), requires_grad=True)\n",
        "\n",
        "for i in range(300):\n",
        "  # Forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=27).float()\n",
        "  xenc = xenc.view(-1, 27*2) # turning each pair into a single flat input vector\n",
        "  # Probs is softmax\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(dim=1, keepdim=True)\n",
        "  # Loss (normalized nll)\n",
        "  loss= -probs[torch.arange(len(xs)), ys].log().mean() + 0.15*(W**2).mean() # with regularization\n",
        "  if i % 20 == 0: print(f\"{i}: {loss.item():.4f}\")\n",
        "  # Backward pass\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "  # Update\n",
        "  W.data += -50 * W.grad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG9ozVol-DXP",
        "outputId": "0d3a823d-87e6-44b0-8cfc-51d723353239"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 4.5354\n",
            "20: 2.4490\n",
            "40: 2.3816\n",
            "60: 2.3606\n",
            "80: 2.3519\n",
            "100: 2.3478\n",
            "120: 2.3457\n",
            "140: 2.3446\n",
            "160: 2.3440\n",
            "180: 2.3436\n",
            "200: 2.3434\n",
            "220: 2.3433\n",
            "240: 2.3432\n",
            "260: 2.3431\n",
            "280: 2.3431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling names\n",
        "names = []\n",
        "for i in range(10):\n",
        "  out = []\n",
        "  ix1, ix2 = 0, 0\n",
        "  while True:\n",
        "    xenc = F.one_hot(torch.tensor([ix1, ix2]), num_classes=27).float()\n",
        "    xenc = xenc.view(-1, 27*2)\n",
        "    logits = xenc @ W\n",
        "    counts = logits.exp()\n",
        "    p = counts / counts.sum(dim=1, keepdim=True)\n",
        "    ix1 = ix2\n",
        "    ix2 = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
        "    out.append(itos[ix2])\n",
        "    if ix2 == 0:\n",
        "      break\n",
        "  names.append(\"\".join(out))\n",
        "\n",
        "print(names)\n",
        "myloss(names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ompFXqW9Ap7-",
        "outputId": "fbba9e30-9f89-4974-bf56-055db46d858b"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ra.', 'aitrettie.', 'zera.', 'xiven.', 'hzie.', 'ellen.', 'ya.', 'dylah.', 'masadacrad.', 'ia.']\n",
            "log_likelihood=tensor(-142.7957)\n",
            "nll=tensor(142.7957)\n",
            "nll/n=tensor(2.4620)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E02: split up the dataset randomly into 80% train set, 10% dev set, 10% test set."
      ],
      "metadata": {
        "id": "CSgz6XRaDfqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "words_train, words_test = train_test_split(words, test_size=0.2, random_state=42)\n",
        "words_dev, words_test = train_test_split(words_test, test_size=0.5, random_state=42)\n",
        "\n",
        "x_train, y_train, x_dev, y_dev, x_test, y_test = [], [], [], [], [], []\n",
        "for wgroup in [words_train, words_dev, words_test]:\n",
        "  xs, ys = [], []\n",
        "  for w in wgroup:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
        "      ix1 = stoi[ch1]\n",
        "      ix2 = stoi[ch2]\n",
        "      ix3 = stoi[ch3]\n",
        "\n",
        "      xs.append([ix1, ix2])\n",
        "      ys.append(ix3)\n",
        "\n",
        "  xs = torch.tensor(xs)\n",
        "  ys = torch.tensor(ys)\n",
        "\n",
        "  if wgroup == words_train:\n",
        "    x_train, y_train = xs, ys\n",
        "  elif wgroup == words_dev:\n",
        "    x_dev, y_dev = xs, ys\n",
        "  else:\n",
        "    x_test, y_test = xs, ys\n"
      ],
      "metadata": {
        "id": "o9mIZgP5DoT7"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27*2, 27), requires_grad=True)\n",
        "\n",
        "for i in range(300):\n",
        "  # Forward pass\n",
        "  xenc = F.one_hot(x_train, num_classes=27).float()\n",
        "  xenc = xenc.view(-1, 27*2) # turning each pair into a single flat input vector\n",
        "  # Probs is softmax\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(dim=1, keepdim=True)\n",
        "  # Loss (normalized nll)\n",
        "  loss= -probs[torch.arange(len(x_train)), y_train].log().mean() + 0.15*(W**2).mean() # with regularization\n",
        "  if i % 20 == 0: print(f\"{i}: {loss.item():.4f}\")\n",
        "  # Backward pass\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "  # Update\n",
        "  W.data += -50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8M33CiAb1vK",
        "outputId": "b1fd60aa-0a58-49b0-ab85-afe6799c5341"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: 4.4906\n",
            "20: 2.4510\n",
            "40: 2.3829\n",
            "60: 2.3616\n",
            "80: 2.3523\n",
            "100: 2.3477\n",
            "120: 2.3453\n",
            "140: 2.3440\n",
            "160: 2.3433\n",
            "180: 2.3429\n",
            "200: 2.3426\n",
            "220: 2.3425\n",
            "240: 2.3424\n",
            "260: 2.3423\n",
            "280: 2.3423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def NN_loss(x, y, W):\n",
        "  xenc = F.one_hot(x, num_classes=27).float()\n",
        "  xenc = xenc.view(-1, 27*2)\n",
        "  # Probs is softmax\n",
        "  logits = xenc @ W\n",
        "  counts = logits.exp()\n",
        "  p = counts / counts.sum(dim=1, keepdim=True)\n",
        "  # Loss = normalized(-nll)\n",
        "  loss= -p[torch.arange(len(x)), y].log().mean()\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "tMQF7QvscybJ"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train Loss: {NN_loss(x_train, y_train, W):.4f}\")\n",
        "print(f\"Dev Loss: {NN_loss(x_dev, y_dev, W):.4f}\")\n",
        "print(f\"Test Loss: {NN_loss(x_test, y_test, W):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOTTRUlmdz9b",
        "outputId": "91ab7b54-c966-4ca3-dd60-48f3359acc66"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.2755\n",
            "Dev Loss: 2.2743\n",
            "Test Loss: 2.2891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E03: Use the dev set to tune the strength of smoothing (or regularization) for the trigram model"
      ],
      "metadata": {
        "id": "j4_foCaPfSKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
        "\n",
        "for i in range(300):\n",
        "  # Forward pass\n",
        "  xenc = F.one_hot(x_train, num_classes=27).float()\n",
        "  xenc = xenc.view(-1, 27*2) # turning each pair into a single flat input vector\n",
        "  # Probs is softmax\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(dim=1, keepdim=True)\n",
        "  # Loss (normalized nll)\n",
        "  loss= -probs[torch.arange(len(x_train)), y_train].log().mean()\n",
        "  #loss+= 0.15*(W**2).mean() # with regularization\n",
        "  if i % 20 == 0: print(f\"{i}: Train Loss: {loss.item():.4f} | Dev Loss {NN_loss(x_dev, y_dev, W):.4f}\")\n",
        "  # Backward pass\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "  # Update\n",
        "  W.data += -50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw1KF7_XfRQN",
        "outputId": "17bf952c-1f0e-4cef-8fee-12cfb8a4dfd7"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: Train Loss: 4.2523 | Dev Loss 4.2462\n",
            "20: Train Loss: 2.3865 | Dev Loss 2.3819\n",
            "40: Train Loss: 2.3079 | Dev Loss 2.3053\n",
            "60: Train Loss: 2.2824 | Dev Loss 2.2807\n",
            "80: Train Loss: 2.2699 | Dev Loss 2.2686\n",
            "100: Train Loss: 2.2624 | Dev Loss 2.2614\n",
            "120: Train Loss: 2.2575 | Dev Loss 2.2567\n",
            "140: Train Loss: 2.2540 | Dev Loss 2.2533\n",
            "160: Train Loss: 2.2513 | Dev Loss 2.2509\n",
            "180: Train Loss: 2.2493 | Dev Loss 2.2490\n",
            "200: Train Loss: 2.2476 | Dev Loss 2.2475\n",
            "220: Train Loss: 2.2463 | Dev Loss 2.2463\n",
            "240: Train Loss: 2.2452 | Dev Loss 2.2453\n",
            "260: Train Loss: 2.2442 | Dev Loss 2.2445\n",
            "280: Train Loss: 2.2434 | Dev Loss 2.2438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best results with no regularization."
      ],
      "metadata": {
        "id": "nJmv0s11h0ld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "E04: Delete our use of F.one_hot in favor of simply indexing into rows of W?"
      ],
      "metadata": {
        "id": "gln-8jOIh4jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
        "\n",
        "for i in range(300):\n",
        "  # Forward pass\n",
        "  # Previous: with one-hot encoding\n",
        "  #xenc = F.one_hot(x_train, num_classes=27).float()\n",
        "  #xenc = xenc.view(-1, 27*2) # turning each pair into a single flat input vector\n",
        "  # Probs is softmax\n",
        "  #logits = xenc @ W # predict log-counts\n",
        "  # Now:\n",
        "  # W is still a weight matrix of shape [54, 27] (2 * 27 inputs â†’ 27 outputs)\n",
        "  # W[xs[:,0]]: selects the corresponding row from the first half of W (rows 0 to 26)\n",
        "  # W[xs[:,1] + 27]: adds 27 to the index, so it selects the second half of W (rows 27 to 53)\n",
        "  logits = W[xs[:,0]] + W[xs[:,1] + 27]\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(dim=1, keepdim=True)\n",
        "  # Loss (normalized nll)\n",
        "  loss= -probs[torch.arange(len(xs)), ys].log().mean()\n",
        "  #loss+= 0.15*(W**2).mean() # with regularization\n",
        "  if i % 20 == 0: print(f\"{i}: Train Loss: {loss.item():.4f} | Dev Loss {NN_loss(x_dev, y_dev, W):.4f}\")\n",
        "  # Backward pass\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "  # Update\n",
        "  W.data += -50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2iLSPr5h3fK",
        "outputId": "1eeda8d6-00f0-46f4-cea3-cfcc840998f4"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: Train Loss: 4.2571 | Dev Loss 4.2720\n",
            "20: Train Loss: 2.3702 | Dev Loss 2.3782\n",
            "40: Train Loss: 2.2985 | Dev Loss 2.3149\n",
            "60: Train Loss: 2.2727 | Dev Loss 2.2935\n",
            "80: Train Loss: 2.2596 | Dev Loss 2.2831\n",
            "100: Train Loss: 2.2516 | Dev Loss 2.2771\n",
            "120: Train Loss: 2.2462 | Dev Loss 2.2733\n",
            "140: Train Loss: 2.2423 | Dev Loss 2.2707\n",
            "160: Train Loss: 2.2394 | Dev Loss 2.2689\n",
            "180: Train Loss: 2.2371 | Dev Loss 2.2676\n",
            "200: Train Loss: 2.2353 | Dev Loss 2.2666\n",
            "220: Train Loss: 2.2338 | Dev Loss 2.2658\n",
            "240: Train Loss: 2.2326 | Dev Loss 2.2652\n",
            "260: Train Loss: 2.2315 | Dev Loss 2.2647\n",
            "280: Train Loss: 2.2306 | Dev Loss 2.2644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "E05: Use F.cross_entropy instead.\n",
        "\n",
        "Compute the cross entropy loss between input logits and target. Better: highly stable, built-in, clean, highly optimized"
      ],
      "metadata": {
        "id": "dNCXIVSpkp-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27*2, 27), generator=g, requires_grad=True)\n",
        "\n",
        "for i in range(300):\n",
        "  # Forward pass\n",
        "  xenc = F.one_hot(xs, num_classes=27).float()\n",
        "  xenc = xenc.view(-1, 27*2) # turning each pair into a single flat input vector\n",
        "  # Probs is softmax\n",
        "  logits = xenc @ W # predict log-counts\n",
        "  counts = logits.exp() # counts, equivalent to N\n",
        "  probs = counts / counts.sum(dim=1, keepdim=True)\n",
        "  # Loss (normalized nll)\n",
        "  loss= torch.nn.functional.cross_entropy(logits, ys)\n",
        "  loss+= 0.15*(W**2).mean() # with regularization\n",
        "  if i % 20 == 0: print(f\"{i}: Train Loss: {loss.item():.4f}\")\n",
        "  # Backward pass\n",
        "  W.grad = None\n",
        "  loss.backward()\n",
        "  # Update\n",
        "  W.data += -50 * W.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KHNkM80l0YV",
        "outputId": "c8c24f59-c9f2-488b-d2c2-0badeb62e109"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: Train Loss: 4.2272\n",
            "20: Train Loss: 2.4433\n",
            "40: Train Loss: 2.3764\n",
            "60: Train Loss: 2.3556\n",
            "80: Train Loss: 2.3468\n",
            "100: Train Loss: 2.3424\n",
            "120: Train Loss: 2.3400\n",
            "140: Train Loss: 2.3388\n",
            "160: Train Loss: 2.3380\n",
            "180: Train Loss: 2.3376\n",
            "200: Train Loss: 2.3373\n",
            "220: Train Loss: 2.3372\n",
            "240: Train Loss: 2.3371\n",
            "260: Train Loss: 2.3370\n",
            "280: Train Loss: 2.3370\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}